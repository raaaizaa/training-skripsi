{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJBL8LpD74Ff"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from transformers import DeiTForImageClassification\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import drive\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_pm6WRwz79Js"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 2\n",
        "NUM_WORKERS = 2\n",
        "MEAN = [0.5, 0.5, 0.5]\n",
        "STD = [0.5, 0.5, 0.5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lrp3jYdd7-WF"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(), \n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(15),                  \n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2), \n",
        "    transforms.ToTensor(),                                    \n",
        "    transforms.Normalize(MEAN, STD)                            \n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.CenterCrop(IMAGE_SIZE), \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxyLEljz8ARe",
        "outputId": "2e91bc26-ec68-42c6-9d79-27c8d1930915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "train_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/dataset-dapa/train/', transform=train_transforms)\n",
        "val_dataset   = datasets.ImageFolder(root='/content/drive/MyDrive/dataset-dapa/val/',   transform=val_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpIZuTrB8Bfn"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-t5IYPA8CtO",
        "outputId": "4578c587-4aef-43a3-cbec-7352ff47d4e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch shape: torch.Size([2, 3, 224, 224])\n",
            "Labels shape: torch.Size([2])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    images, labels = next(iter(train_loader))\n",
        "    print(f\"Batch shape: {images.shape}\") \n",
        "    print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "model = DeiTForImageClassification.from_pretrained(\n",
        "    \"facebook/deit-base-distilled-patch16-224\",\n",
        "    num_labels=9,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "model.deit.requires_grad_(False)\n",
        "\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(model.classifier.in_features, 9)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ9dGpRB8D3R",
        "outputId": "3a4cb4b7-84a5-4ea5-b941-a43822c62c20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device:  cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('device: ', device)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.1, patience=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpmQUHzD8HDf"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += images.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += images.size(0)\n",
        "\n",
        "    return val_loss / total, correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3eShBbv8IKj",
        "outputId": "e7eea235-c570-47db-e1bb-4fb84adc6b12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30: Train loss 0.6044, acc 0.7966 | Val   loss 0.2977, acc 0.9091\n",
            "Epoch 2/30: Train loss 0.3566, acc 0.8774 | Val   loss 0.2357, acc 0.9182\n",
            "Epoch 3/30: Train loss 0.3035, acc 0.8904 | Val   loss 0.1917, acc 0.9323\n",
            "Epoch 4/30: Train loss 0.2747, acc 0.9006 | Val   loss 0.1862, acc 0.9333\n",
            "Epoch 5/30: Train loss 0.2806, acc 0.9038 | Val   loss 0.1720, acc 0.9424\n",
            "Epoch 6/30: Train loss 0.2700, acc 0.9032 | Val   loss 0.1555, acc 0.9424\n",
            "Epoch 7/30: Train loss 0.2874, acc 0.9040 | Val   loss 0.1701, acc 0.9384\n",
            "Epoch 8/30: Train loss 0.2700, acc 0.9055 | Val   loss 0.1493, acc 0.9404\n",
            "Epoch 9/30: Train loss 0.2603, acc 0.9066 | Val   loss 0.1611, acc 0.9354\n",
            "Epoch 10/30: Train loss 0.2630, acc 0.9094 | Val   loss 0.1658, acc 0.9394\n",
            "Epoch 11/30: Train loss 0.2705, acc 0.9049 | Val   loss 0.1222, acc 0.9566\n",
            "Epoch 12/30: Train loss 0.2719, acc 0.9051 | Val   loss 0.1427, acc 0.9505\n",
            "Epoch 13/30: Train loss 0.2744, acc 0.9083 | Val   loss 0.1569, acc 0.9455\n",
            "Epoch 14/30: Train loss 0.2749, acc 0.9073 | Val   loss 0.1629, acc 0.9434\n",
            "Epoch 15/30: Train loss 0.2625, acc 0.9122 | Val   loss 0.1285, acc 0.9535\n",
            "Epoch 16/30: Train loss 0.2753, acc 0.9140 | Val   loss 0.1555, acc 0.9525\n",
            "Epoch 17/30: Train loss 0.2906, acc 0.9116 | Val   loss 0.1468, acc 0.9475\n",
            "Epoch 18/30: Train loss 0.2779, acc 0.9066 | Val   loss 0.1379, acc 0.9515\n",
            "Epoch 19/30: Train loss 0.2615, acc 0.9166 | Val   loss 0.1919, acc 0.9364\n",
            "Epoch 20/30: Train loss 0.2554, acc 0.9174 | Val   loss 0.1613, acc 0.9465\n",
            "Epoch 21/30: Train loss 0.2705, acc 0.9159 | Val   loss 0.1473, acc 0.9515\n",
            "Epoch 22/30: Train loss 0.2783, acc 0.9135 | Val   loss 0.1588, acc 0.9394\n",
            "Epoch 23/30: Train loss 0.2739, acc 0.9096 | Val   loss 0.1559, acc 0.9414\n",
            "Epoch 24/30: Train loss 0.2598, acc 0.9144 | Val   loss 0.1260, acc 0.9525\n",
            "Epoch 25/30: Train loss 0.2783, acc 0.9099 | Val   loss 0.1693, acc 0.9414\n",
            "Epoch 26/30: Train loss 0.2669, acc 0.9185 | Val   loss 0.1729, acc 0.9333\n",
            "Epoch 27/30: Train loss 0.2707, acc 0.9122 | Val   loss 0.1664, acc 0.9354\n",
            "Epoch 28/30: Train loss 0.2586, acc 0.9192 | Val   loss 0.1555, acc 0.9444\n",
            "Epoch 29/30: Train loss 0.2677, acc 0.9189 | Val   loss 0.1313, acc 0.9535\n",
            "Epoch 30/30: Train loss 0.2774, acc 0.9163 | Val   loss 0.1773, acc 0.9364\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc     = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "          f\"Train loss {train_loss:.4f}, acc {train_acc:.4f} | \"\n",
        "          f\"Val   loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"DeiT.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLC_ahve8JDJ",
        "outputId": "bc2bc52a-ceda-4979-d557-26453cf57dbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved successfully to Google Drive!\n"
          ]
        }
      ],
      "source": [
        "save_path = '/content/drive/MyDrive/DeiT.pth'\n",
        "\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(\"Model saved successfully to Google Drive!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn2HWzZ98KJQ",
        "outputId": "01bf7b59-ba04-403b-b389-400197fb0555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved at: /content/drive/MyDrive/models/DeiT.pth\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/models'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "save_path = os.path.join(save_dir, 'DeiT.pth')\n",
        "\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model saved at: {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWAyCfnjm7ZQ",
        "outputId": "221b0fe7-82f7-461c-ab66-b1b7f40d6daa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Overall Accuracy: 93.64%\n",
            "\n",
            "Class                     Precision  Recall     F1-Score   Class Acc \n",
            "----------------------------------------------------------------------\n",
            "algal_spot                0.99       0.96       0.97       95.86     \n",
            "brown_blight              0.97       0.80       0.88       79.70     \n",
            "gray_blight               0.85       0.98       0.91       97.55     \n",
            "healthy                   0.99       0.88       0.93       88.00     \n",
            "helopeltis                0.91       0.99       0.95       98.67     \n",
            "red-rust                  1.00       0.96       0.98       95.65     \n",
            "red-spider-infested       1.00       1.00       1.00       100.00    \n",
            "red_spot                  0.93       0.98       0.95       97.66     \n",
            "white-spot                0.91       1.00       0.95       100.00    \n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"\\nOverall Accuracy: {accuracy:.2f}%\\n\")\n",
        "\n",
        "report = classification_report(all_labels, all_preds, target_names=val_dataset.classes, output_dict=True)\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
        "\n",
        "print(f\"{'Class':<25} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Class Acc':<10}\")\n",
        "print(\"-\" * 70)\n",
        "for idx, class_name in enumerate(val_dataset.classes):\n",
        "    cls_report = report[class_name]\n",
        "    precision = cls_report['precision']\n",
        "    recall = cls_report['recall']\n",
        "    f1 = cls_report['f1-score']\n",
        "    acc = per_class_accuracy[idx]\n",
        "    print(f\"{class_name:<25} {precision:<10.2f} {recall:<10.2f} {f1:<10.2f} {acc*100:<10.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-mXscbsm3me",
        "outputId": "7484037b-59ba-4cc2-c7ee-6c29514795af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "         algal_spot     0.9878    0.9586    0.9730       169\n",
            "       brown_blight     0.9725    0.7970    0.8760       133\n",
            "        gray_blight     0.8503    0.9755    0.9086       163\n",
            "            healthy     0.9851    0.8800    0.9296       150\n",
            "         helopeltis     0.9080    0.9867    0.9457       150\n",
            "           red-rust     1.0000    0.9565    0.9778        23\n",
            "red-spider-infested     1.0000    1.0000    1.0000        21\n",
            "           red_spot     0.9330    0.9766    0.9543       171\n",
            "         white-spot     0.9091    1.0000    0.9524        10\n",
            "\n",
            "           accuracy                         0.9364       990\n",
            "          macro avg     0.9495    0.9479    0.9464       990\n",
            "       weighted avg     0.9409    0.9364    0.9359       990\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'algal_spot': {'precision': 0.9878048780487805,\n",
              "  'recall': 0.9585798816568047,\n",
              "  'f1-score': 0.972972972972973,\n",
              "  'support': 169.0},\n",
              " 'brown_blight': {'precision': 0.9724770642201835,\n",
              "  'recall': 0.7969924812030075,\n",
              "  'f1-score': 0.8760330578512396,\n",
              "  'support': 133.0},\n",
              " 'gray_blight': {'precision': 0.8502673796791443,\n",
              "  'recall': 0.9754601226993865,\n",
              "  'f1-score': 0.9085714285714286,\n",
              "  'support': 163.0},\n",
              " 'healthy': {'precision': 0.9850746268656716,\n",
              "  'recall': 0.88,\n",
              "  'f1-score': 0.9295774647887324,\n",
              "  'support': 150.0},\n",
              " 'helopeltis': {'precision': 0.9079754601226994,\n",
              "  'recall': 0.9866666666666667,\n",
              "  'f1-score': 0.9456869009584664,\n",
              "  'support': 150.0},\n",
              " 'red-rust': {'precision': 1.0,\n",
              "  'recall': 0.9565217391304348,\n",
              "  'f1-score': 0.9777777777777777,\n",
              "  'support': 23.0},\n",
              " 'red-spider-infested': {'precision': 1.0,\n",
              "  'recall': 1.0,\n",
              "  'f1-score': 1.0,\n",
              "  'support': 21.0},\n",
              " 'red_spot': {'precision': 0.9329608938547486,\n",
              "  'recall': 0.9766081871345029,\n",
              "  'f1-score': 0.9542857142857143,\n",
              "  'support': 171.0},\n",
              " 'white-spot': {'precision': 0.9090909090909091,\n",
              "  'recall': 1.0,\n",
              "  'f1-score': 0.9523809523809523,\n",
              "  'support': 10.0},\n",
              " 'accuracy': 0.9363636363636364,\n",
              " 'macro avg': {'precision': 0.9495168013202373,\n",
              "  'recall': 0.9478698976100893,\n",
              "  'f1-score': 0.9463651410652538,\n",
              "  'support': 990.0},\n",
              " 'weighted avg': {'precision': 0.9408654462702581,\n",
              "  'recall': 0.9363636363636364,\n",
              "  'f1-score': 0.9358860428297228,\n",
              "  'support': 990.0}}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/DeiT.pth'))\n",
        "model.to(device)\n",
        "\n",
        "class_names = val_dataset.classes\n",
        "\n",
        "evaluate_model(model, val_loader, device, class_names=class_names)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
