{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MikFJD5QASUw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from transformers import DeiTForImageClassification\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import drive\n",
        "from transformers import BeitFeatureExtractor, BeitForImageClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HuY42_DBM1s"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 2\n",
        "NUM_WORKERS = 2\n",
        "MEAN = [0.5, 0.5, 0.5]\n",
        "STD = [0.5, 0.5, 0.5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKjkMizfBQIs"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD)\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.CenterCrop(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukoWGyS5BSZz",
        "outputId": "c828507f-2102-4a09-d9f0-bee330525f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "train_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/dataset-dapa/train/', transform=train_transforms)\n",
        "val_dataset   = datasets.ImageFolder(root='/content/drive/MyDrive/dataset-dapa/val/',   transform=val_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn3OIfeXBVt6"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLYX0YqxBbjR",
        "outputId": "e113032b-a9d2-4223-a4ff-e939a81ecca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch shape: torch.Size([2, 3, 224, 224])\n",
            "Labels shape: torch.Size([2])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-base-patch16-224-pt22k-ft22k and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([21841, 768]) in the checkpoint and torch.Size([9, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    images, labels = next(iter(train_loader))\n",
        "    print(f\"Batch shape: {images.shape}\")\n",
        "    print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "model = BeitForImageClassification.from_pretrained(\n",
        "    \"microsoft/beit-base-patch16-224-pt22k-ft22k\",\n",
        "    num_labels=9,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "model.beit.requires_grad_(False)\n",
        "\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(model.classifier.in_features, 9)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRTvJL8WBs0B"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q93bvWVMBvxs"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += images.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += images.size(0)\n",
        "\n",
        "    return val_loss / total, correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYRkZp6sB0WG",
        "outputId": "5e5b731d-bb2b-44e8-9a07-865902c3ac7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15: Train loss 0.3556, acc 0.8833 | Val   loss 0.1505, acc 0.9515\n",
            "Epoch 2/15: Train loss 0.1890, acc 0.9345 | Val   loss 0.2175, acc 0.9323\n",
            "Epoch 3/15: Train loss 0.1717, acc 0.9423 | Val   loss 0.1411, acc 0.9556\n",
            "Epoch 4/15: Train loss 0.1727, acc 0.9399 | Val   loss 0.1098, acc 0.9687\n",
            "Epoch 5/15: Train loss 0.1766, acc 0.9438 | Val   loss 0.1291, acc 0.9616\n",
            "Epoch 6/15: Train loss 0.1587, acc 0.9479 | Val   loss 0.0976, acc 0.9707\n",
            "Epoch 7/15: Train loss 0.1379, acc 0.9535 | Val   loss 0.1095, acc 0.9707\n",
            "Epoch 8/15: Train loss 0.1518, acc 0.9498 | Val   loss 0.1699, acc 0.9505\n",
            "Epoch 9/15: Train loss 0.1701, acc 0.9449 | Val   loss 0.1455, acc 0.9606\n",
            "Epoch 10/15: Train loss 0.1705, acc 0.9492 | Val   loss 0.1655, acc 0.9596\n",
            "Epoch 11/15: Train loss 0.1893, acc 0.9466 | Val   loss 0.1072, acc 0.9768\n",
            "Epoch 12/15: Train loss 0.1714, acc 0.9498 | Val   loss 0.1408, acc 0.9758\n",
            "Epoch 13/15: Train loss 0.1487, acc 0.9531 | Val   loss 0.1101, acc 0.9697\n",
            "Epoch 14/15: Train loss 0.1569, acc 0.9535 | Val   loss 0.1492, acc 0.9677\n",
            "Epoch 15/15: Train loss 0.1623, acc 0.9544 | Val   loss 0.1772, acc 0.9626\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 15\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc     = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "          f\"Train loss {train_loss:.4f}, acc {train_acc:.4f} | \"\n",
        "          f\"Val   loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"BEiT-15epoch.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMSpx_Zwor2I",
        "outputId": "d1f610ba-b39d-4ad7-fdd6-6261b0f1f066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved successfully to Google Drive!\n"
          ]
        }
      ],
      "source": [
        "save_path = '/content/drive/MyDrive/BEiT-15epoch.pth'\n",
        "\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(\"Model saved successfully to Google Drive!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zekHSHv4oerl",
        "outputId": "2a4768cc-ace4-46d0-c16d-2b4a3e373d14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved at: /content/drive/MyDrive/models/BEiT-15epoch.pth\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/models'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "save_path = os.path.join(save_dir, 'BEiT-15epoch.pth')\n",
        "\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model saved at: {save_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
