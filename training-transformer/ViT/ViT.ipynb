{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MikFJD5QASUw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from transformers import ViTForImageClassification\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "MEAN = [0.5, 0.5, 0.5]\n",
        "STD = [0.5, 0.5, 0.5]"
      ],
      "metadata": {
        "id": "3HuY42_DBM1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD)\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.CenterCrop(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD)\n",
        "])"
      ],
      "metadata": {
        "id": "DKjkMizfBQIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "train_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/dataset-dapa/train/', transform=train_transforms)\n",
        "val_dataset   = datasets.ImageFolder(root='/content/drive/MyDrive/dataset-dapa/val/',   transform=val_transforms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukoWGyS5BSZz",
        "outputId": "e53fd53a-bdfc-4d28-e82f-27327218191c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "Zn3OIfeXBVt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    images, labels = next(iter(train_loader))\n",
        "    print(f\"Batch shape: {images.shape}\")\n",
        "    print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224\",\n",
        "    num_labels=9,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "model.vit.requires_grad_(False)\n",
        "\n",
        "\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(model.classifier.in_features, 9)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLYX0YqxBbjR",
        "outputId": "e98aedbb-53ed-4155-eb45-cd249ec89511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch shape: torch.Size([8, 3, 224, 224])\n",
            "Labels shape: torch.Size([8])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([9, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.1, patience=3,\n",
        ")"
      ],
      "metadata": {
        "id": "SRTvJL8WBs0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += images.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += images.size(0)\n",
        "\n",
        "    return val_loss / total, correct / total"
      ],
      "metadata": {
        "id": "Q93bvWVMBvxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc     = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "          f\"Train loss {train_loss:.4f}, acc {train_acc:.4f} | \"\n",
        "          f\"Val   loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"ViT.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYRkZp6sB0WG",
        "outputId": "9edeb6ff-b467-420e-b7e5-7b53f77ccc75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30: Train loss 0.5262, acc 0.8281 | Val   loss 0.3018, acc 0.8939\n",
            "Epoch 2/30: Train loss 0.2613, acc 0.9133 | Val   loss 0.2469, acc 0.9101\n",
            "Epoch 3/30: Train loss 0.2324, acc 0.9196 | Val   loss 0.2005, acc 0.9364\n",
            "Epoch 4/30: Train loss 0.2082, acc 0.9261 | Val   loss 0.1869, acc 0.9354\n",
            "Epoch 5/30: Train loss 0.1880, acc 0.9330 | Val   loss 0.2202, acc 0.9263\n",
            "Epoch 6/30: Train loss 0.2048, acc 0.9293 | Val   loss 0.1760, acc 0.9354\n",
            "Epoch 7/30: Train loss 0.1745, acc 0.9328 | Val   loss 0.1576, acc 0.9424\n",
            "Epoch 8/30: Train loss 0.1692, acc 0.9377 | Val   loss 0.1533, acc 0.9475\n",
            "Epoch 9/30: Train loss 0.1712, acc 0.9351 | Val   loss 0.1789, acc 0.9444\n",
            "Epoch 10/30: Train loss 0.1827, acc 0.9347 | Val   loss 0.1821, acc 0.9404\n",
            "Epoch 11/30: Train loss 0.1589, acc 0.9401 | Val   loss 0.1723, acc 0.9424\n",
            "Epoch 12/30: Train loss 0.1649, acc 0.9401 | Val   loss 0.1611, acc 0.9434\n",
            "Epoch 13/30: Train loss 0.1722, acc 0.9397 | Val   loss 0.1500, acc 0.9404\n",
            "Epoch 14/30: Train loss 0.1725, acc 0.9360 | Val   loss 0.1340, acc 0.9505\n",
            "Epoch 15/30: Train loss 0.1665, acc 0.9393 | Val   loss 0.1448, acc 0.9394\n",
            "Epoch 16/30: Train loss 0.1599, acc 0.9442 | Val   loss 0.1610, acc 0.9424\n",
            "Epoch 17/30: Train loss 0.1729, acc 0.9401 | Val   loss 0.1367, acc 0.9515\n",
            "Epoch 18/30: Train loss 0.1488, acc 0.9470 | Val   loss 0.1752, acc 0.9404\n",
            "Epoch 19/30: Train loss 0.1613, acc 0.9436 | Val   loss 0.1455, acc 0.9545\n",
            "Epoch 20/30: Train loss 0.1558, acc 0.9412 | Val   loss 0.1938, acc 0.9364\n",
            "Epoch 21/30: Train loss 0.1663, acc 0.9403 | Val   loss 0.1440, acc 0.9444\n",
            "Epoch 22/30: Train loss 0.1761, acc 0.9364 | Val   loss 0.1595, acc 0.9485\n",
            "Epoch 23/30: Train loss 0.1732, acc 0.9410 | Val   loss 0.1323, acc 0.9485\n",
            "Epoch 24/30: Train loss 0.1587, acc 0.9453 | Val   loss 0.1872, acc 0.9364\n",
            "Epoch 25/30: Train loss 0.1589, acc 0.9431 | Val   loss 0.2252, acc 0.9354\n",
            "Epoch 26/30: Train loss 0.1610, acc 0.9444 | Val   loss 0.1414, acc 0.9495\n",
            "Epoch 27/30: Train loss 0.1649, acc 0.9444 | Val   loss 0.1453, acc 0.9475\n",
            "Epoch 28/30: Train loss 0.1741, acc 0.9408 | Val   loss 0.1617, acc 0.9525\n",
            "Epoch 29/30: Train loss 0.1575, acc 0.9429 | Val   loss 0.1736, acc 0.9444\n",
            "Epoch 30/30: Train loss 0.1566, acc 0.9466 | Val   loss 0.1328, acc 0.9616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/MyDrive/ViT.pth'\n",
        "\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(\"Model saved successfully to Google Drive!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMSpx_Zwor2I",
        "outputId": "a916ad20-876c-4c1c-88fa-7ff2d98283cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully to Google Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/models'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "save_path = os.path.join(save_dir, 'ViT.pth')\n",
        "\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model saved at: {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zekHSHv4oerl",
        "outputId": "691ca4a7-1451-4cfb-f100-4d37b5e46845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved at: /content/drive/MyDrive/models/ViT.pth\n"
          ]
        }
      ]
    }
  ]
}