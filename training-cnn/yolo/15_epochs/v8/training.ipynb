{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "9b2f905b",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from ultralytics import YOLO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "fe5fa1a3",
            "metadata": {},
            "outputs": [],
            "source": [
                "NUM_CLASSES = 9\n",
                "NUM_WORKERS = 2\n",
                "NUM_EPOCHS = 15\n",
                "\n",
                "BATCH_SIZE = 8\n",
                "IMAGE_SIZE = 224\n",
                "LEARNING_RATE = 1e-3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "7e50bf77",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-cls.pt to 'yolov8n-cls.pt'...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 5.31M/5.31M [00:00<00:00, 5.57MB/s]\n"
                    ]
                }
            ],
            "source": [
                "pretrained_model_path = 'yolov8n-cls.pt'\n",
                "model = YOLO(pretrained_model_path, task='classify')\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "model = model.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "4011817e",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_path = '../../../../dataset'\n",
                "name = 'yolov8_cls'\n",
                "project_path = './runs/train'\n",
                "\n",
                "training_args = {\n",
                "    # train configuration\n",
                "    'data': dataset_path,\n",
                "    'project': project_path,\n",
                "    'name': name,\n",
                "\n",
                "    'pretrained': True,\n",
                "    'optimizer': 'Adam',\n",
                "    'mode': 'train',\n",
                "    'device': device,\n",
                "    'task': 'classify',\n",
                "    'exist_ok': True,\n",
                "\n",
                "    'epochs': NUM_EPOCHS,\n",
                "    'workers': NUM_WORKERS,\n",
                "    'batch': BATCH_SIZE,\n",
                "    'lr0': LEARNING_RATE,\n",
                "    'patience': 50,\n",
                "\n",
                "    # hyperparameter setting\n",
                "    'imgsz': IMAGE_SIZE,\n",
                "    'hsv_v': 0.2,\n",
                "    'degrees': 15.0,\n",
                "    'translate': 0.1,\n",
                "    'scale': 0.5,\n",
                "    'shear': 0.0,\n",
                "    'flipud': 0.5,\n",
                "    'fliplr': 0.5,\n",
                "    'mosaic': 0.0,\n",
                "    'mixup': 0.0,\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "6207da03",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "New https://pypi.org/project/ultralytics/8.3.174 available 😃 Update with 'pip install -U ultralytics'\n",
                        "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=../../../../dataset, degrees=15.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=15, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.5, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.2, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-cls.pt, momentum=0.937, mosaic=0.0, multi_scale=False, name=yolov8_cls, nbs=64, nms=False, opset=None, optimize=False, optimizer=Adam, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=./runs/train, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/train/yolov8_cls, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
                        "\u001b[34m\u001b[1mtrain:\u001b[0m /home/oz31/code/personal/python/training-skripsi/dataset/train... found 4626 images in 9 classes ✅ \n",
                        "\u001b[34m\u001b[1mval:\u001b[0m /home/oz31/code/personal/python/training-skripsi/dataset/val... found 990 images in 9 classes ✅ \n",
                        "\u001b[34m\u001b[1mtest:\u001b[0m /home/oz31/code/personal/python/training-skripsi/dataset/test... found 995 images in 9 classes ✅ \n",
                        "Overriding model.yaml nc=1000 with nc=9\n",
                        "\n",
                        "                   from  n    params  module                                       arguments                     \n",
                        "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
                        "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
                        "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
                        "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
                        "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
                        "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
                        "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
                        "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
                        "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
                        "  9                  -1  1    341769  ultralytics.nn.modules.head.Classify         [256, 9]                      \n",
                        "YOLOv8n-cls summary: 56 layers, 1,449,817 parameters, 1,449,817 gradients, 3.4 GFLOPs\n",
                        "Transferred 156/158 items from pretrained weights\n",
                        "WARNING ⚠️ \u001b[34m\u001b[1mAMP: \u001b[0mchecks failed ❌. AMP training on NVIDIA GeForce GTX 1650 SUPER GPU may cause NaN losses or zero-mAP results, so AMP will be disabled during training.\n",
                        "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 384.3±8.6 MB/s, size: 149.8 KB)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/oz31/code/personal/python/training-skripsi/dataset/train... 4626 images, 0 corrupt: 100%|██████████| 4626/4626 [00:00<?, ?it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 321.3±44.8 MB/s, size: 174.1 KB)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/oz31/code/personal/python/training-skripsi/dataset/val... 990 images, 0 corrupt: 100%|██████████| 990/990 [00:00<?, ?it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001, momentum=0.937) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
                        "Image sizes 224 train, 224 val\n",
                        "Using 2 dataloader workers\n",
                        "Logging results to \u001b[1mruns/train/yolov8_cls\u001b[0m\n",
                        "Starting training for 15 epochs...\n",
                        "\n",
                        "      Epoch    GPU_mem       loss  Instances       Size\n",
                        "                   all      0.819      0.997\n",
                        "      Epoch    GPU_mem       loss  Instances       Size\n",
                        "                   all      0.903      0.998\n",
                        "      Epoch    GPU_mem       loss  Instances       Size\n",
                        "                   all      0.922      0.999\n",
                        "      Epoch    GPU_mem       loss  Instances       Size\n",
                        "                   all      0.931      0.999\n",
                        "      Epoch    GPU_mem       loss  Instances       Size\n",
                        "                   all       0.94          1\n",
                        "      Epoch    GPU_mem       loss  Instances       Size\n",
                        "                   all      0.945          1\n",
                        "      Epoch    GPU_mem       loss  Instances       Size\n",
                        "                   all      0.944          1\n",
                        "      Epoch    GPU_mem       loss  Instances       Size\n",
                        "                   all      0.945          1\n",
                        "      Epoch    GPU_mem       loss  Instances       Size\n",
                        "                   all       0.95          1\n",
                        "      Epoch    GPU_mem       loss  Instances       Size\n",
                        "                   all      0.952          1\n",
                        "      Epoch    GPU_mem       loss  Instances       Size\n",
                        "                   all      0.955          1\n",
                        "      Epoch    GPU_mem       loss  Instances       Size\n",
                        "                   all      0.955          1\n",
                        "      Epoch    GPU_mem       loss  Instances       Size\n",
                        "                   all      0.954          1\n",
                        "      Epoch    GPU_mem       loss  Instances       Size\n",
                        "                   all      0.956          1\n",
                        "      Epoch    GPU_mem       loss  Instances       Size\n",
                        "                   all      0.955          1\n",
                        "\n",
                        "15 epochs completed in 0.118 hours.\n",
                        "Optimizer stripped from runs/train/yolov8_cls/weights/last.pt, 3.0MB\n",
                        "Optimizer stripped from runs/train/yolov8_cls/weights/best.pt, 3.0MB\n",
                        "\n",
                        "Validating runs/train/yolov8_cls/weights/best.pt...\n",
                        "Ultralytics 8.3.134 🚀 Python-3.12.3 torch-2.7.0+cu126 CUDA:0 (NVIDIA GeForce GTX 1650 SUPER, 3875MiB)\n",
                        "YOLOv8n-cls summary (fused): 30 layers, 1,446,409 parameters, 0 gradients, 3.3 GFLOPs\n",
                        "WARNING ⚠️ Dataset 'split=train' not found at /home/oz31/code/personal/python/cnn/raw/train\n",
                        "Found 6575 images in subdirectories. Attempting to split...\n",
                        "Splitting /home/oz31/code/personal/python/cnn/raw (9 classes, 6611 images) into 80% train, 20% val...\n",
                        "Split complete in /home/oz31/code/personal/python/cnn/raw_split ✅\n",
                        "\u001b[34m\u001b[1mtrain:\u001b[0m /home/oz31/code/personal/python/cnn/raw_split/train... found 6337 images in 9 classes ✅\n",
                        "\u001b[34m\u001b[1mval:\u001b[0m /home/oz31/code/personal/python/cnn/raw_split/val... found 2374 images in 9 classes ✅\n",
                        "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
                        "                   all      0.956          1\n",
                        "Speed: 0.1ms preprocess, 0.3ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
                        "Results saved to \u001b[1mruns/train/yolov8_cls\u001b[0m\n",
                        "result model train YOLO v8: ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
                        "\n",
                        "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7590ce5867e0>\n",
                        "curves: []\n",
                        "curves_results: []\n",
                        "fitness: 0.9780966639518738\n",
                        "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
                        "results_dict: {'metrics/accuracy_top1': 0.9561933279037476, 'metrics/accuracy_top5': 1.0, 'fitness': 0.9780966639518738}\n",
                        "save_dir: PosixPath('runs/train/yolov8_cls')\n",
                        "speed: {'preprocess': 0.056235829139271824, 'inference': 0.3450473080404325, 'loss': 0.00026161406819463854, 'postprocess': 0.0003646954832868679}\n",
                        "task: 'classify'\n",
                        "top1: 0.9561933279037476\n",
                        "top5: 1.0\n"
                    ]
                }
            ],
            "source": [
                "results = model.train(**training_args)\n",
                "print(f\"result model train YOLO v8: {results}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv (3.12.3)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
