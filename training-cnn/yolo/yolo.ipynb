{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transform\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAugmentedDataset(ImageFolder):\n",
    "\n",
    "    # constructor\n",
    "    def __init__(self, root, transforms_dict, default_transforms=None):\n",
    "        super().__init__(root)\n",
    "        self.transform_dict = transforms_dict\n",
    "        self.default_transform = default_transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "\n",
    "        class_name = self.classes[target]\n",
    "        transform = self.transform_dict.get(class_name, self.default_transform)\n",
    "\n",
    "        if transform:\n",
    "            sample = transform(sample)\n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class detected: ['algal_spot', 'brown-blight', 'gray-blight', 'healthy', 'helopeltis', 'leaf-rust', 'red-rust', 'red-spider-infested', 'red-spot', 'white-spot']\n",
      "Augmentation summary per class:\n",
      "algal_spot      ‚Üí Default\n",
      "brown-blight    ‚Üí Default\n",
      "gray-blight     ‚Üí Default\n",
      "healthy         ‚Üí Default\n",
      "helopeltis      ‚Üí Default\n",
      "leaf-rust       ‚Üí Default\n",
      "red-rust        ‚Üí Default\n",
      "red-spider-infested ‚Üí Default\n",
      "red-spot        ‚Üí Strong\n",
      "white-spot      ‚Üí Default\n"
     ]
    }
   ],
   "source": [
    "default_transform = transform.Compose([\n",
    "    transform.Resize(256),\n",
    "    transform.CenterCrop(224),\n",
    "    transform.ToTensor(),\n",
    "    transform.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "strong_transform = transform.Compose([\n",
    "    transform.Resize(256),\n",
    "    transform.RandomResizedCrop(224),\n",
    "    transform.RandomHorizontalFlip(),\n",
    "    transform.RandomRotation(30),\n",
    "    transform.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transform.ToTensor(),\n",
    "    transform.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "small_classes = ['red-spot']\n",
    "transform_dict = {cls: strong_transform for cls in small_classes}\n",
    "\n",
    "# load dataset beserta transform-nya\n",
    "train_dataset = CustomAugmentedDataset(root='/home/oz31/code/personal/python/tea/dataset/Train/', transforms_dict=transform_dict, default_transforms=default_transform)\n",
    "val_dataset = CustomAugmentedDataset(root='/home/oz31/code/personal/python/tea/dataset/Valid/', transforms_dict=transform_dict, default_transforms=default_transform)\n",
    "\n",
    "class_counts = np.bincount([label for _, label in train_dataset.samples])\n",
    "class_weight = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
    "\n",
    "sample_weights = [class_weight[label] for _, label in train_dataset.samples]\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, sampler=sampler, num_workers=6, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, num_workers=4, shuffle=False)\n",
    "\n",
    "print(f\"Class detected: {train_dataset.classes}\")\n",
    "\n",
    "print(\"Augmentation summary per class:\")\n",
    "for cls in train_dataset.classes:\n",
    "    print(f\"{cls.ljust(15)} ‚Üí {'Strong' if cls in transform_dict else 'Default'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO11 classification model with pre-trained weights\n",
    "model = YOLO(\"yolo11n-cls.pt\")  # Nano classification model, pre-trained on ImageNet\n",
    "# Note: YOLO11 handles classifier adjustment internally during training, no manual layer modification needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is cuda available? True\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('is cuda available?', torch.cuda.is_available())\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer (for fine-tuning, applied to YOLO‚Äôs parameters)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.112 üöÄ Python-3.12.3 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce GTX 1650 SUPER, 3875MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolo11n-cls.pt, data=../../datasets/Train/, epochs=30, time=None, patience=50, batch=8, imgsz=224, save=True, save_period=-1, cache=False, device=0, workers=0, project=./runs/train, name=yolo11_cls_exp, exist_ok=True, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=10.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=0.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/train/yolo11_cls_exp\n",
      "\n",
      "WARNING ‚ö†Ô∏è Dataset not found, missing path /home/oz31/code/personal/python/training-skripsi/training-cnn/datasets/Train, attempting download...\n",
      "Downloading https://ultralytics.com/assets/../../datasets/Train.zip to '/home/oz31/code/personal/python/training-skripsi/training-cnn/datasets/Train.zip'...\n",
      "WARNING ‚ö†Ô∏è Download failure, retrying 1/3 https://ultralytics.com/assets/../../datasets/Train.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#=#=#                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset download success ‚úÖ (1.1s), saved to \u001b[1m/home/oz31/code/personal/python/training-skripsi/training-cnn/datasets/Train\u001b[0m\n",
      "\n",
      "WARNING ‚ö†Ô∏è Dataset 'split=train' not found at /home/oz31/code/personal/python/training-skripsi/training-cnn/datasets/Train/train\n",
      "ERROR ‚ùå No images found in /home/oz31/code/personal/python/training-skripsi/training-cnn/datasets/Train or its subdirectories.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset '../../datasets/Train' error ‚ùå [Errno 2] No such file or directory: '/home/oz31/code/personal/python/training-skripsi/training-cnn/datasets/Train/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:574\u001b[39m, in \u001b[36mBaseTrainer.get_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.task == \u001b[33m\"\u001b[39m\u001b[33mclassify\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m     data = \u001b[43mcheck_cls_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.data.split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33myaml\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33myml\u001b[39m\u001b[33m\"\u001b[39m} \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.task \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[32m    576\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdetect\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    577\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msegment\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    578\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    579\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mobb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    580\u001b[39m }:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/ultralytics/data/utils.py:538\u001b[39m, in \u001b[36mcheck_cls_dataset\u001b[39m\u001b[34m(dataset, split)\u001b[39m\n\u001b[32m    537\u001b[39m nc = \u001b[38;5;28mlen\u001b[39m([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (data_dir / \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m).glob(\u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m x.is_dir()])  \u001b[38;5;66;03m# number of classes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m names = \u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# class names list\u001b[39;00m\n\u001b[32m    539\u001b[39m names = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28msorted\u001b[39m(names)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/pathlib.py:1058\u001b[39m, in \u001b[36mPath.iterdir\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1053\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Yield path objects of the directory contents.\u001b[39;00m\n\u001b[32m   1054\u001b[39m \n\u001b[32m   1055\u001b[39m \u001b[33;03mThe children are yielded in arbitrary order, and the\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[33;03mspecial entries '.' and '..' are not included.\u001b[39;00m\n\u001b[32m   1057\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1058\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m   1059\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_child_relpath(name)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/oz31/code/personal/python/training-skripsi/training-cnn/datasets/Train/train'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m      3\u001b[39m training_args = {\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33m../../datasets/Train/\u001b[39m\u001b[33m'\u001b[39m,  \u001b[38;5;66;03m# Correct dataset path\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m: num_epochs,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmixup\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.0\u001b[39m,    \u001b[38;5;66;03m# Disable mixup\u001b[39;00m\n\u001b[32m     29\u001b[39m }\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtotal batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m best_val_loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/ultralytics/engine/model.py:784\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.get(\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    782\u001b[39m     args[\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.ckpt_path\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer = \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrainer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args.get(\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[32m    786\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/ultralytics/models/yolo/classify/train.py:73\u001b[39m, in \u001b[36mClassificationTrainer.__init__\u001b[39m\u001b[34m(self, cfg, overrides, _callbacks)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overrides.get(\u001b[33m\"\u001b[39m\u001b[33mimgsz\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     72\u001b[39m     overrides[\u001b[33m\"\u001b[39m\u001b[33mimgsz\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[32m224\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:137\u001b[39m, in \u001b[36mBaseTrainer.__init__\u001b[39m\u001b[34m(self, cfg, overrides, _callbacks)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28mself\u001b[39m.model = check_model_file_from_stem(\u001b[38;5;28mself\u001b[39m.args.model)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch_distributed_zero_first(LOCAL_RANK):  \u001b[38;5;66;03m# avoid auto-downloading dataset multiple times\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainset, \u001b[38;5;28mself\u001b[39m.testset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28mself\u001b[39m.ema = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# Optimization utils init\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:585\u001b[39m, in \u001b[36mBaseTrainer.get_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    583\u001b[39m             \u001b[38;5;28mself\u001b[39m.args.data = data[\u001b[33m\"\u001b[39m\u001b[33myaml_file\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# for validating 'yolo train data=url.zip' usage\u001b[39;00m\n\u001b[32m    584\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m585\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(emojis(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_url(\u001b[38;5;28mself\u001b[39m.args.data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m error ‚ùå \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    586\u001b[39m \u001b[38;5;28mself\u001b[39m.data = data\n\u001b[32m    587\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.single_cls:\n",
      "\u001b[31mRuntimeError\u001b[39m: Dataset '../../datasets/Train' error ‚ùå [Errno 2] No such file or directory: '/home/oz31/code/personal/python/training-skripsi/training-cnn/datasets/Train/train'"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "training_args = {\n",
    "    'data': '../../dataset/Train/',  # Correct dataset path\n",
    "    'epochs': num_epochs,\n",
    "    'imgsz': 224,  # Match ResNet\n",
    "    'batch': 8,\n",
    "    'device': 0 if torch.cuda.is_available() else 'cpu',\n",
    "    'workers': 0,  # Match your setup\n",
    "    'project': './runs/train',\n",
    "    'name': 'yolo11_cls_exp',\n",
    "    'exist_ok': True,\n",
    "    'pretrained': True,\n",
    "    'optimizer': 'Adam',\n",
    "    'lr0': 0.001,\n",
    "    'patience': 50,\n",
    "    # Augmentation settings (respecting red-spot's strong augmentation)\n",
    "    'hsv_h': 0.015,  # Default hue\n",
    "    'hsv_s': 0.7,    # Default saturation\n",
    "    'hsv_v': 0.4,    # Default value\n",
    "    'degrees': 10.0,  # Rotation\n",
    "    'translate': 0.1, # Translation\n",
    "    'scale': 0.5,    # Zoom\n",
    "    'shear': 0.0,\n",
    "    'flipud': 0.0,   # Vertical flip\n",
    "    'fliplr': 0.5,   # Horizontal flip\n",
    "    'mosaic': 0.0,   # Disable mosaic for classification\n",
    "    'mixup': 0.0,    # Disable mixup\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "results = model.train(**training_args)\n",
    "\n",
    "print(f\"total batches: {len(train_dataloader)}\")\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"starting epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_dataloader):\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"processing batch {batch_idx+1}/{len(train_dataloader)}\")\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    train_accuracy = (correct / total) * 100\n",
    "    print(f\"epoch {epoch+1}/{num_epochs}, loss: {avg_loss:.4f}, accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_accuracy = (val_correct / val_total) * 100\n",
    "    print(f\"validation loss: {avg_val_loss:.4f}, accuracy: {val_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
