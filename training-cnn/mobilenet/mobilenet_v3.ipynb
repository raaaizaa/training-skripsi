{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2ad0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transform\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1824cafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAugmentedDataset(ImageFolder):\n",
    "\n",
    "    # constructor\n",
    "    def __init__(self, root, transforms_dict, default_transforms=None):\n",
    "        super().__init__(root)\n",
    "        self.transform_dict = transforms_dict\n",
    "        self.default_transform = default_transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "\n",
    "        class_name = self.classes[target]\n",
    "        transform = self.transform_dict.get(class_name, self.default_transform)\n",
    "\n",
    "        if transform:\n",
    "            sample = transform(sample)\n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7322098f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class detected: ['algal_spot', 'brown-blight', 'gray-blight', 'healthy', 'helopeltis', 'leaf-rust', 'red-rust', 'red-spider-infested', 'red-spot', 'white-spot']\n",
      "Augmentation summary per class:\n",
      "algal_spot      → Default\n",
      "brown-blight    → Default\n",
      "gray-blight     → Default\n",
      "healthy         → Default\n",
      "helopeltis      → Default\n",
      "leaf-rust       → Default\n",
      "red-rust        → Default\n",
      "red-spider-infested → Default\n",
      "red-spot        → Strong\n",
      "white-spot      → Default\n"
     ]
    }
   ],
   "source": [
    "default_transform = transform.Compose([\n",
    "    transform.Resize(256),\n",
    "    transform.CenterCrop(224),\n",
    "    transform.ToTensor(),\n",
    "    transform.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "strong_transform = transform.Compose([\n",
    "    transform.Resize(256),\n",
    "    transform.RandomResizedCrop(224),\n",
    "    transform.RandomHorizontalFlip(),\n",
    "    transform.RandomRotation(30),\n",
    "    transform.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transform.ToTensor(),\n",
    "    transform.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "small_classes = ['red-spot']\n",
    "transform_dict = {cls: strong_transform for cls in small_classes}\n",
    "\n",
    "# load dataset beserta transform-nya\n",
    "train_dataset = CustomAugmentedDataset(root='/home/oz31/code/personal/python/tea/dataset/Train/', transforms_dict=transform_dict, default_transforms=default_transform)\n",
    "val_dataset = CustomAugmentedDataset(root='/home/oz31/code/personal/python/tea/dataset/Valid/', transforms_dict=transform_dict, default_transforms=default_transform)\n",
    "\n",
    "class_counts = np.bincount([label for _, label in train_dataset.samples])\n",
    "class_weight = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
    "\n",
    "sample_weights = [class_weight[label] for _, label in train_dataset.samples]\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, sampler=sampler, num_workers=6, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, num_workers=4, shuffle=False)\n",
    "\n",
    "print(f\"Class detected: {train_dataset.classes}\")\n",
    "\n",
    "print(\"Augmentation summary per class:\")\n",
    "for cls in train_dataset.classes:\n",
    "    print(f\"{cls.ljust(15)} → {'Strong' if cls in transform_dict else 'Default'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cfa235f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV3(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): Conv2dNormActivation(\n",
       "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
       "    (1): Hardswish()\n",
       "    (2): Dropout(p=0.2, inplace=True)\n",
       "    (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# import model\n",
    "# with pre-trained weights\n",
    "model = models.mobilenet_v3_small()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30f8ce8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is cuda available? True\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(train_dataset.classes)\n",
    "# ganti layer terakhir biar disesuain sama jumlah class di dataset\n",
    "# model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes) # ADJUST SESUAI MODEL\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('is cuda available?', torch.cuda.is_available())\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88ca7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function buat klasifikasi multi-class\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92ffddfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total batches: 1241\n",
      "starting epoch 1/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 1/30, loss: 1.6873, accuracy: 44.02%\n",
      "validation loss: 1.5679, accuracy: 44.27%\n",
      "starting epoch 2/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 2/30, loss: 1.0023, accuracy: 60.48%\n",
      "validation loss: 1.3818, accuracy: 48.09%\n",
      "starting epoch 3/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 3/30, loss: 0.8518, accuracy: 66.38%\n",
      "validation loss: 1.0175, accuracy: 58.60%\n",
      "starting epoch 4/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 4/30, loss: 0.7384, accuracy: 71.16%\n",
      "validation loss: 1.0768, accuracy: 55.73%\n",
      "starting epoch 5/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 5/30, loss: 0.6591, accuracy: 74.36%\n",
      "validation loss: 1.2851, accuracy: 54.30%\n",
      "starting epoch 6/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 6/30, loss: 0.5899, accuracy: 77.73%\n",
      "validation loss: 1.2326, accuracy: 52.23%\n",
      "starting epoch 7/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 7/30, loss: 0.5254, accuracy: 79.98%\n",
      "validation loss: 0.8762, accuracy: 70.86%\n",
      "starting epoch 8/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 8/30, loss: 0.4876, accuracy: 81.81%\n",
      "validation loss: 1.2759, accuracy: 58.60%\n",
      "starting epoch 9/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 9/30, loss: 0.4460, accuracy: 83.08%\n",
      "validation loss: 1.0072, accuracy: 66.56%\n",
      "starting epoch 10/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 10/30, loss: 0.4281, accuracy: 84.10%\n",
      "validation loss: 0.9629, accuracy: 68.95%\n",
      "starting epoch 11/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 11/30, loss: 0.3897, accuracy: 85.64%\n",
      "validation loss: 0.8974, accuracy: 69.75%\n",
      "starting epoch 12/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 12/30, loss: 0.3460, accuracy: 87.37%\n",
      "validation loss: 0.9859, accuracy: 69.27%\n",
      "starting epoch 13/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 13/30, loss: 0.3334, accuracy: 88.37%\n",
      "validation loss: 1.0419, accuracy: 67.20%\n",
      "starting epoch 14/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 14/30, loss: 0.3213, accuracy: 88.60%\n",
      "validation loss: 1.1379, accuracy: 64.97%\n",
      "starting epoch 15/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 15/30, loss: 0.3084, accuracy: 88.86%\n",
      "validation loss: 1.2173, accuracy: 63.54%\n",
      "starting epoch 16/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 16/30, loss: 0.2724, accuracy: 90.37%\n",
      "validation loss: 0.9989, accuracy: 69.90%\n",
      "starting epoch 17/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 17/30, loss: 0.2804, accuracy: 90.26%\n",
      "validation loss: 1.2393, accuracy: 66.40%\n",
      "starting epoch 18/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 18/30, loss: 0.2538, accuracy: 90.74%\n",
      "validation loss: 1.3068, accuracy: 65.92%\n",
      "starting epoch 19/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 19/30, loss: 0.2468, accuracy: 91.32%\n",
      "validation loss: 1.0757, accuracy: 68.47%\n",
      "starting epoch 20/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 20/30, loss: 0.2478, accuracy: 91.01%\n",
      "validation loss: 1.1512, accuracy: 66.40%\n",
      "starting epoch 21/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 21/30, loss: 0.2330, accuracy: 91.71%\n",
      "validation loss: 0.9922, accuracy: 72.29%\n",
      "starting epoch 22/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 22/30, loss: 0.2056, accuracy: 92.93%\n",
      "validation loss: 1.2363, accuracy: 72.93%\n",
      "starting epoch 23/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 23/30, loss: 0.2155, accuracy: 92.65%\n",
      "validation loss: 1.1490, accuracy: 71.34%\n",
      "starting epoch 24/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 24/30, loss: 0.1993, accuracy: 93.05%\n",
      "validation loss: 1.1672, accuracy: 70.70%\n",
      "starting epoch 25/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 25/30, loss: 0.2001, accuracy: 92.92%\n",
      "validation loss: 1.0851, accuracy: 75.00%\n",
      "starting epoch 26/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 26/30, loss: 0.1942, accuracy: 93.35%\n",
      "validation loss: 1.2261, accuracy: 72.29%\n",
      "starting epoch 27/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 27/30, loss: 0.1958, accuracy: 92.68%\n",
      "validation loss: 1.1842, accuracy: 69.59%\n",
      "starting epoch 28/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 28/30, loss: 0.1839, accuracy: 93.62%\n",
      "validation loss: 1.1606, accuracy: 71.18%\n",
      "starting epoch 29/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 29/30, loss: 0.1832, accuracy: 93.46%\n",
      "validation loss: 1.1985, accuracy: 73.25%\n",
      "starting epoch 30/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 30/30, loss: 0.1784, accuracy: 93.88%\n",
      "validation loss: 1.1593, accuracy: 70.22%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "print(f\"total batches: {len(train_dataloader)}\")\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"starting epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_dataloader):\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"processing batch {batch_idx+1}/{len(train_dataloader)}\")\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    train_accuracy = (correct / total) * 100\n",
    "    print(f\"epoch {epoch+1}/{num_epochs}, loss: {avg_loss:.4f}, accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_accuracy = (val_correct / val_total) * 100\n",
    "    print(f\"validation loss: {avg_val_loss:.4f}, accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "503bc6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"mobilenet_v3.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
