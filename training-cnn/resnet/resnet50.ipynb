{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec1b51fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transform\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a36be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAugmentedDataset(ImageFolder):\n",
    "    \n",
    "    # constructor: folder dataset, transform_dict: nama class & augmentasi khusus\n",
    "    def __init__(self, root, transform_dict, default_transform=None):\n",
    "        super().__init__(root)\n",
    "        self.transform_dict = transform_dict\n",
    "        self.default_transform = default_transform\n",
    "    \n",
    "    # ambil gambar dari class, apply augmentasi, return gambar hasil augmentasi\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "    \n",
    "        class_name = self.classes[target]\n",
    "        transform = self.transform_dict.get(class_name, self.default_transform)\n",
    "\n",
    "        if transform:\n",
    "            sample = transform(sample)\n",
    "        \n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a63242a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes detected: ['algal_spot', 'brown-blight', 'gray-blight', 'healthy', 'helopeltis', 'leaf-rust', 'red-rust', 'red-spider-infested', 'red-spot', 'white-spot']\n",
      "Augmentation summary per class:\n",
      "algal_spot      → Default\n",
      "brown-blight    → Default\n",
      "gray-blight     → Default\n",
      "healthy         → Default\n",
      "helopeltis      → Default\n",
      "leaf-rust       → Default\n",
      "red-rust        → Default\n",
      "red-spider-infested → Default\n",
      "red-spot        → Strong\n",
      "white-spot      → Default\n"
     ]
    }
   ],
   "source": [
    "default_transform = transform.Compose([\n",
    "    transform.Resize(256),\n",
    "    transform.CenterCrop(224),\n",
    "    transform.ToTensor(),\n",
    "    transform.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "strong_transform = transform.Compose([\n",
    "    transform.Resize(256),\n",
    "    transform.RandomResizedCrop(224),\n",
    "    transform.RandomHorizontalFlip(),\n",
    "    transform.RandomRotation(30),\n",
    "    transform.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transform.ToTensor(),\n",
    "    transform.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# deteksi augmentasi khusus buat red_spot\n",
    "small_classes = ['red-spot']\n",
    "transform_dict = {cls: strong_transform for cls in small_classes}\n",
    "\n",
    "# load dataset beserta transofrmer function\n",
    "train_dataset = CustomAugmentedDataset(root='/home/oz31/code/personal/python/tea/dataset/Train/', transform_dict=transform_dict, default_transform=default_transform)\n",
    "val_dataset = CustomAugmentedDataset(root='/home/oz31/code/personal/python/tea/dataset/Valid/', transform_dict=transform_dict, default_transform=default_transform)\n",
    "\n",
    "class_counts = np.bincount([label for _, label in train_dataset.samples])\n",
    "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
    "\n",
    "# sampler buat ngatasin class imbalance dengan cara ngasih probabilitas sampling lebih tinggi ke class yang punya data sedikit\n",
    "sample_weights = [class_weights[label] for _, label in train_dataset.samples]\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, sampler=sampler, num_workers=6, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Classes detected: {train_dataset.classes}\")\n",
    "\n",
    "print(\"Augmentation summary per class:\")\n",
    "for cls in train_dataset.classes:\n",
    "    print(f\"{cls.ljust(15)} → {'Strong' if cls in transform_dict else 'Default'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fa67963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import model\n",
    "# with pre-trained weights\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9548ec6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is cuda available? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('is cuda available?', torch.cuda.is_available())\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5c69844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function buat klasifikasi multi-class\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4301894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total batches: 1241\n",
      "starting epoch 1/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 1/30, loss: 0.5628, accuracy: 83.37%\n",
      "validation loss: 0.7097, accuracy: 79.30%\n",
      "starting epoch 2/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 2/30, loss: 0.2804, accuracy: 90.20%\n",
      "validation loss: 0.7224, accuracy: 84.55%\n",
      "starting epoch 3/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 3/30, loss: 0.2279, accuracy: 91.98%\n",
      "validation loss: 0.5106, accuracy: 83.44%\n",
      "starting epoch 4/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 4/30, loss: 0.2024, accuracy: 92.83%\n",
      "validation loss: 0.7727, accuracy: 78.18%\n",
      "starting epoch 5/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 5/30, loss: 0.1947, accuracy: 93.45%\n",
      "validation loss: 0.5333, accuracy: 85.67%\n",
      "starting epoch 6/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 6/30, loss: 0.1758, accuracy: 94.25%\n",
      "validation loss: 0.5813, accuracy: 83.44%\n",
      "starting epoch 7/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 7/30, loss: 0.1440, accuracy: 95.03%\n",
      "validation loss: 0.5443, accuracy: 85.03%\n",
      "starting epoch 8/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 8/30, loss: 0.1544, accuracy: 94.75%\n",
      "validation loss: 0.5569, accuracy: 82.64%\n",
      "starting epoch 9/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 9/30, loss: 0.1391, accuracy: 95.20%\n",
      "validation loss: 0.6718, accuracy: 81.37%\n",
      "starting epoch 10/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 10/30, loss: 0.1225, accuracy: 95.91%\n",
      "validation loss: 0.5904, accuracy: 84.08%\n",
      "starting epoch 11/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 11/30, loss: 0.1285, accuracy: 95.41%\n",
      "validation loss: 0.6069, accuracy: 81.53%\n",
      "starting epoch 12/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 12/30, loss: 0.1150, accuracy: 96.05%\n",
      "validation loss: 0.6741, accuracy: 81.69%\n",
      "starting epoch 13/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 13/30, loss: 0.1074, accuracy: 96.02%\n",
      "validation loss: 0.7751, accuracy: 81.37%\n",
      "starting epoch 14/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 14/30, loss: 0.1131, accuracy: 96.03%\n",
      "validation loss: 0.7397, accuracy: 80.25%\n",
      "starting epoch 15/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 15/30, loss: 0.1025, accuracy: 96.47%\n",
      "validation loss: 0.5682, accuracy: 84.08%\n",
      "starting epoch 16/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 16/30, loss: 0.1007, accuracy: 96.30%\n",
      "validation loss: 0.6483, accuracy: 78.18%\n",
      "starting epoch 17/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 17/30, loss: 0.0919, accuracy: 96.65%\n",
      "validation loss: 0.8647, accuracy: 80.41%\n",
      "starting epoch 18/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 18/30, loss: 0.1066, accuracy: 96.16%\n",
      "validation loss: 0.6152, accuracy: 81.21%\n",
      "starting epoch 19/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 19/30, loss: 0.0872, accuracy: 96.64%\n",
      "validation loss: 0.7290, accuracy: 80.10%\n",
      "starting epoch 20/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 20/30, loss: 0.1038, accuracy: 96.06%\n",
      "validation loss: 0.6524, accuracy: 81.37%\n",
      "starting epoch 21/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 21/30, loss: 0.0886, accuracy: 96.43%\n",
      "validation loss: 0.8247, accuracy: 83.12%\n",
      "starting epoch 22/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 22/30, loss: 0.0846, accuracy: 96.72%\n",
      "validation loss: 0.6855, accuracy: 79.46%\n",
      "starting epoch 23/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 23/30, loss: 0.0867, accuracy: 96.85%\n",
      "validation loss: 0.7678, accuracy: 81.53%\n",
      "starting epoch 24/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 24/30, loss: 0.0997, accuracy: 96.27%\n",
      "validation loss: 0.6530, accuracy: 82.01%\n",
      "starting epoch 25/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 25/30, loss: 0.0788, accuracy: 96.96%\n",
      "validation loss: 0.6478, accuracy: 82.32%\n",
      "starting epoch 26/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 26/30, loss: 0.0833, accuracy: 96.97%\n",
      "validation loss: 0.8002, accuracy: 83.92%\n",
      "starting epoch 27/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 27/30, loss: 0.0800, accuracy: 96.76%\n",
      "validation loss: 0.7887, accuracy: 82.48%\n",
      "starting epoch 28/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 28/30, loss: 0.0900, accuracy: 96.47%\n",
      "validation loss: 0.6880, accuracy: 78.82%\n",
      "starting epoch 29/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 29/30, loss: 0.0838, accuracy: 96.45%\n",
      "validation loss: 0.6895, accuracy: 81.53%\n",
      "starting epoch 30/30\n",
      "processing batch 1/1241\n",
      "processing batch 101/1241\n",
      "processing batch 201/1241\n",
      "processing batch 301/1241\n",
      "processing batch 401/1241\n",
      "processing batch 501/1241\n",
      "processing batch 601/1241\n",
      "processing batch 701/1241\n",
      "processing batch 801/1241\n",
      "processing batch 901/1241\n",
      "processing batch 1001/1241\n",
      "processing batch 1101/1241\n",
      "processing batch 1201/1241\n",
      "epoch 30/30, loss: 0.0789, accuracy: 96.92%\n",
      "validation loss: 0.7350, accuracy: 79.46%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "print(f\"total batches: {len(train_dataloader)}\")\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"starting epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_dataloader):\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"processing batch {batch_idx+1}/{len(train_dataloader)}\")\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    train_accuracy = (correct / total) * 100\n",
    "    print(f\"epoch {epoch+1}/{num_epochs}, loss: {avg_loss:.4f}, accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_accuracy = (val_correct / val_total) * 100\n",
    "    print(f\"validation loss: {avg_val_loss:.4f}, accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67039f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"resnet50.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
