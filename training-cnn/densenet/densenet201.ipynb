{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transform\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAugmentedDataset(ImageFolder):\n",
    "    \n",
    "    # root: folder dataset, transform_dict: nama class & augmentasi khusus\n",
    "    def __init__(self, root, transform_dict):\n",
    "        super().__init__(root)\n",
    "        self.transform_dict = transform_dict\n",
    "    \n",
    "    # ambil gambar dari class, apply augmentasi, return gambar hasil augmentasi\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "    \n",
    "        class_name = self.classes[target]\n",
    "        transform = self.transform_dict.get(class_name, self.transform)\n",
    "\n",
    "        if transform:\n",
    "            sample = transform(sample)\n",
    "        \n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_transform = transform.Compose([\n",
    "    transform.Resize(256),\n",
    "    transform.CenterCrop(224),\n",
    "    transform.ToTensor(),\n",
    "    transform.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                 std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "strong_transform = transform.Compose([\n",
    "    transform.Resize(256),\n",
    "    transform.RandomResizedCrop(224),\n",
    "    transform.RandomHorizontalFlip(),\n",
    "    transform.RandomRotation(30),\n",
    "    transform.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transform.ToTensor(),\n",
    "    transform.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                 std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deteksi augmentasi khusus buat red spot\n",
    "small_classes = ['red-spot']\n",
    "transform_dict = {cls: strong_transform for cls in small_classes}\n",
    "\n",
    "# load dataset beserta transform-nya\n",
    "train_dataset = CustomAugmentedDataset(root='../../dataset/Train/', transform_dict=transform_dict)\n",
    "val_dataset = CustomAugmentedDataset(root='../../dataset/Valid/', transform_dict=transform_dict)\n",
    "\n",
    "# class count\n",
    "class_counts = np.bincount([label for _, label in train_dataset.samples])\n",
    "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
    "\n",
    "# sampler buat ngatasin class imbalance dengan cara ngasih probabilitas sampling lebih tinggi ke class yang punya data sedikit\n",
    "sample_weights = [class_weights[label] for _, label in train_dataset.samples]\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, sampler=sampler, num_workers=6, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Classes detected: {train_dataset.classes}\")\n",
    "\n",
    "print(\"Augmentation summary per class:\")\n",
    "for cls in train_dataset.classes:\n",
    "    print(f\"{cls.ljust(15)} â†’ {'Strong' if cls in transform_dict else 'Default'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "model = models.vgg19(pretrained=True) # GANTI SESUAI MODEL YANG MAU DI-TRAIN\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# ganti layer terakhir biar disesuain sama jumlah class di dataset\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes) # ADJUST SESUAI MODEL\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('is cuda available?', torch.cuda.is_available())\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function buat klasifikasi multi-class\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "print(f\"Total batches: {len(train_dataloader)}\")\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_dataloader):\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Processing batch {batch_idx+1}/{len(train_dataloader)}\")\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    train_accuracy = (correct / total) * 100\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_accuracy = (val_correct / val_total) * 100\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"vggnet19.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
